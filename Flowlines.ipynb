{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c003275",
   "metadata": {},
   "source": [
    "# Plot Flowines Using Remote Greenland Ice Mapping Project Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81053f22",
   "metadata": {},
   "source": [
    "This notebook demonstrates how Greenland Ice Mapping Project can be remotely accessed to create plots along flowlines from [Felikson et al., 2020](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112), which are archived on [Zenodo](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2020GL090112). The copies of the shapefiles included in this repository were downloaded in late January 2022. The notebook works with a specific set of glacier ids but is easily modified to plot results for other glaciers in the flowline data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64595cba-d36e-44ce-b7f6-85a61b0f23c7",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926fc4b-d655-4824-9ede-d9b8e5f81764",
   "metadata": {},
   "source": [
    "The following packages are needed to execute this notebook. The notebook has been tested with the `environment.yml` in the *binder* folder of this repository. Thus, for best results, create a new conda environment to run this and other other GrIMP notebooks from this repository. \n",
    "\n",
    "`conda env create -f binder/environment.yml`\n",
    "\n",
    "`conda activate greenlandMapping`\n",
    "\n",
    "`python -m ipykernel install --user --name=greenlandMapping`\n",
    "\n",
    "`jupyter lab`\n",
    "\n",
    "See [NSIDCLoginNotebook](https://github.com/fastice/GrIMPNotebooks/blob/master/NSIDCLoginNotebook.ipynb) for additional information.\n",
    "\n",
    "The notebooks can be run on a temporary virtial instance (to start click [**binder**](https://mybinder.org/v2/gh/fastice/GrIMPNotebooks/HEAD?urlpath=lab)). See the github [README](https://github.com/fastice/GrIMPNotebooks#readme) for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f31bba-ea54-46c0-b066-4ceb0f3a6497",
   "metadata": {},
   "source": [
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9874e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import nisardev as nisar\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import grimpfunc as grimp\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import importlib\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "dask.config.set(num_workers=2)  # Avoid problems with too many open connections at NSIDC\n",
    "import glob\n",
    "import panel\n",
    "panel.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882700b-58fe-4162-86fc-41bb62e3a566",
   "metadata": {},
   "source": [
    "**Note to get help and see options for any of the GrIMP or other functions while the cursor is positioned inside a method's parentheses, click shift+Tab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970afc4",
   "metadata": {},
   "source": [
    "## Read Shapefiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b4ebe",
   "metadata": {},
   "source": [
    "In the examples presented here we will use glaciers 1 through 6 in the Felikson data base, which should have downloaded as part of the notebook repository. Glacier 6 has two variants, so some extra work is needed to extract. Each glacier's flowlines are used to great `grimp.Flowlines` instances, which are saved in a dictionary, `myFlowlines` with glacier id: '0001' through 'b006'. To limit the plots to the downstream regions, the flowlines are all truncated to a `length` of 50km. Within each myFlowines entry (a `grimp.Flowlines` instance), the individual flowlines are maintained as a dictionary `myFlowlines['glacierId'].flowlines`. The line with `glob`, which effectively is an `ls` command can be modified to select other glaciers. Minor tweaks may be needed to optimize the plots generated below to accomodate the different geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf700ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "myShapeFiles = []\n",
    "for i in range(1, 7):\n",
    "    myShapeFiles += glob.glob(f'./shpfiles/glacier?00{i}.shp')  # Search for glaciers with ?00n where n ranges from 1 to 6\n",
    "# Build dictionary with flowlines indexed by glacier id\n",
    "myFlowlines = {x[-8:-4]: grimp.Flowlines(shapefile=x, name=x[-8:-4], length=50e3) for x in myShapeFiles}\n",
    "myFlowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f765f55d",
   "metadata": {},
   "source": [
    "The last cell created a dictionary of flowline objects, indexed by glacier id (e.g., '0001'...). Each object contains a dictionary with several flowlines for that glacier indexed by flowline index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c43dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598b2ba",
   "metadata": {},
   "source": [
    "The information for each flowline contains a dictionary the x, y coordinates and distance along the flowline, d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f11d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFlowlines['0001'].flowlines['03'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0841102",
   "metadata": {},
   "source": [
    "The area of interest can be defined as the union of the bounds for all of the flowlines computed as shown below along with the unique set of flowline IDs across all glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "myBounds = {'minx': 1e9, 'miny': 1e9, 'maxx': -1e9, 'maxy': -1e9}  # Initial bounds to force reset\n",
    "flowlineIDs = []  #\n",
    "for myKey in myFlowlines:\n",
    "    myBounds = myFlowlines[myKey].mergeBounds(myBounds, myFlowlines[myKey].bounds)\n",
    "    flowlineIDs.append(myFlowlines[myKey].flowlineIDs())\n",
    "flowlineIDs = np.unique(flowlineIDs)\n",
    "print(myBounds)\n",
    "print(flowlineIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc001e20",
   "metadata": {},
   "source": [
    "## Search Catalog for Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99638546",
   "metadata": {},
   "source": [
    "For remote access to data at NSIDC, run these cells to login with your NASA EarthData Login (see  [NSIDCLoginNotebook](https://github.com/fastice/GrIMPNotebooks/blob/master/NSIDCLoginNotebook.ipynb) for further details). These cells can skipped if all data are being accessed locally. First define where the cookie files need for login are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7381580",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dict(GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/.grimp_download_cookiejar.txt'),\n",
    "            GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/.grimp_download_cookiejar.txt'))\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8eac6",
   "metadata": {},
   "source": [
    "Now enter credentials. If previous valid cookie exists, login credentials can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "myLogin = grimp.NASALogin()\n",
    "myLogin.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6948b955",
   "metadata": {},
   "source": [
    "The next cell searches the NSIDC database for the desired full Sentinel-1 based GRiMP velocity mosaics. For purposes of demonstration, it comes up with a search result for annual mosaics. Use the controls to search for data ranging from 6-day to annual resolution. Note download times scale with temporal resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a822dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some environments the tool is unresponsive (i.e., search button doesn't work) - this can often be fixed by re-running this cell\n",
    "myUrls = grimp.cmrUrls(mode='nisar')  # nisar mode excludes image and tsx products and allows only one product type at a time\n",
    "myUrls.initialSearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94aa71-7a44-44b8-890e-31bf0ff9bbba",
   "metadata": {},
   "source": [
    "## To Chunk or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952f0aa3-2941-4057-998b-750f71140253",
   "metadata": {},
   "source": [
    "There are two ways to read the data, which are controlled by the parameter `useStack`:\n",
    "1) `useStack=True` (default): In this case, each band of the subsetted data is loaded as a single file read operation (e.g., no chunks in xy). For most cases, this is the faster option because there is no dask overhead and the single reads tend to be faster. This is the preferred option for anything that involves reading in data with `loadRemote` and doing repeated operations on the result. **Note: with this option, the keyword `chunks` will be ignored.**\n",
    "2) `useStack=False`: In this case, the data are chunked with rio-xarray and dask, which can add 10s of seconds or more overhead to set up the xarray, which slows the lazy reads. There are only a few use cases in which this mode would be preferred. For example, examining only a few points in a large subset **without** using `loadRemote`, so that as the data are input on the fly, only the chunks surrounding the points would be read. In most cases, it's best to minimize the subset area (e.g., for one glacier) or use multiple subsets for widely spaced glaciers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f59297-7b80-4ae7-a600-96bef000d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "useStack=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd90760d-2e7b-46b2-b812-ff3fb568b3c5",
   "metadata": {},
   "source": [
    "## Number of Workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b4084d-2e0d-4442-8385-1ae5b84b82ea",
   "metadata": {},
   "source": [
    "With Dask you can use `numWorkers` to specify multiple parallel threads, which can speed up downloads. It can also cause the download to fail (with something that looks like a file not found error) if the server decides it's receiving too many concurrent requests. The criteria under which this happens are unclear, and whether it has to do with the number of connections, open files, etc. This means the results could differ for the type of data product/access. For example, downloading a large number of TSX files (many file open operations per unit time) might cause things to break before the case where large pieces are pulled from full ice sheet mosaics (files are held open for long periods). Beyond `numWorkers=8`, the point of diminishing returns is approached. In general, `numWorkers=4` will be fairly robust, and provide good performance. But you can experiment by setting `numWorkers` below. Note: this discussion is most applicable to network reads. For local file systems, the speedup may be substantially less due to file contention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30607182-82c7-46be-9f73-ef07ceec545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numWorkers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9283fa2f",
   "metadata": {},
   "source": [
    "## Load the Velocity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340b2d4",
   "metadata": {},
   "source": [
    "While the velocity data are stored as multiple files at NSIDC, they can all be combined into a single `nisarVelSeries` instance. This instance will set all the data structures up, but will not initially download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries = nisar.nisarVelSeries(numWorkers=numWorkers) # Create Series\n",
    "urlNames = [x.replace('vv','*').replace('.tif','') for x in myUrls.getCogs()] # Add wild card and remove trailing tiff\n",
    "myVelSeries.readSeriesFromTiff(urlNames, url=True, readSpeed=False, useStack=useStack)\n",
    "myVelSeries.xr  # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6950a592",
   "metadata": {},
   "source": [
    "For the annual data set, this step produces a ~7GB data sets, which expands to 370GB for the full data set. To avoid downloading unnessary data, the data can be subsetted using the bounding box from the flowlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a905c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.subsetVel(myBounds) # Apply subset\n",
    "myVelSeries.subset # Add semicolon after to suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efb0f2",
   "metadata": {},
   "source": [
    "The volume of the data set is now a far more manageable ~29MB, which is still located in the archive.  Operations can continue without downloading, but if lots of operations are going to occur, it is best to download the data upfront. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "myVelSeries.loadRemote() # Load the data to memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf07712",
   "metadata": {},
   "source": [
    "## Display Flowlines and Velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3e2f8",
   "metadata": {},
   "source": [
    "The flowlines over one of the velocity layers can be displayed as with the following block of code. In addition to plotting the flowline, a point 10 km along each flowline is plotted and saved for subsequent plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure and axis\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,12))\n",
    "glacierPoints = {}\n",
    "# generate a color dict that spans all flowline ids, using method from a flowline instance\n",
    "flowlineColors = list(myFlowlines.values())[0].genColorDict(flowlineIDs=flowlineIDs)\n",
    "# Messy plot stuff to get glacier labels in right place\n",
    "xShift = {x: 1 for x in myFlowlines} # Dict index by glacier id\n",
    "xShift['0001'] = -2 # custom for jakobshavn to keep on plot\n",
    "# Display the velocity map for 2020 (note picks map nearest the date given).\n",
    "# Saturate color table at 2000 to maintain slower detail (vmax=2000)\n",
    "myVelSeries.displayVelForDate('2020-01-01', ax=ax, labelFontSize=10, plotFontSize=9, titleFontSize=14,\n",
    "                              vmin=0, vmax=2000, units='km')\n",
    "# Look over each glacier and plot the flowlines\n",
    "for glacierId in myFlowlines:\n",
    "    # Plot the flowline Match units to the map\n",
    "    myFlowlines[glacierId].plotFlowlineLocations(ax=ax, units='km', colorDict=flowlineColors)\n",
    "    #\n",
    "    myFlowlines[glacierId].plotGlacierName(ax=ax, units='km', xShift=xShift[glacierId],\n",
    "                                           color='w', fontsize=12,fontweight='bold', first=False)\n",
    "    # Generates points 10km\n",
    "    points10km = myFlowlines[glacierId].extractPoints(10, None, units='km')\n",
    "    glacierPoints[glacierId] = points10km\n",
    "    for key in points10km:\n",
    "        ax.plot(*points10km[key], 'r.')\n",
    "# Create a dict of unique labels for legend\n",
    "h, l = ax.get_legend_handles_labels()\n",
    "by_label = dict(zip(l, h))\n",
    "# Add legend\n",
    "ax.legend(by_label.values(), by_label.keys(), title='Flowline ID', ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ff2ba",
   "metadata": {},
   "source": [
    "## Plot Central Flowlines at Different Times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7aed6",
   "metadata": {},
   "source": [
    "This example will demonstrate plotting the nominally central flowline ('06') for each of the six years for which there are currently data. This example works for annual data. Minor modifications are needed for the legend if more frequent data are selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowlineId ='06'  # Flowline id to plot\n",
    "fig, axes = plt.subplots(np.ceil(len(myFlowlines)/4).astype(int), 4, figsize=(18, 9))  # Setup plot\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "    # return interpolated values as vx(time index, distance index)\n",
    "    # vx, vy, vv = myVelSeries.interp(*myFlowlines[glacierId].xy(units='km'), units='km')\n",
    "    # loop over each profile by time\n",
    "    for myDate in myVelSeries.time:\n",
    "        # new plot method\n",
    "        myVelSeries.plotProfile(*myFlowlines[glacierId].xy(index=flowlineId, units='km'), date=myDate, label=myDate.year, units='km', ax=ax)\n",
    "        # ax.plot(myFlowlines[glacierId].flowlineDistance(units='km'), speed, label=myDate.year)\n",
    "    # pretty up plot\n",
    "    ax.legend(ncol=2, loc='upper right')\n",
    "    #ax.set_xlabel('Distance (km)', fontsize=13)\n",
    "    #ax.set_ylabel('Speed (m/yr)', fontsize=13,)\n",
    "    myVelSeries.labelProfilePlot(ax, title=f'Glacier {glacierId}')\n",
    "    #ax.set_title(f'Glacier {glacierId}')\n",
    "for ax in axes.flatten()[len(myFlowlines):]:\n",
    "    ax.axis('off'); # kill axis with no plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00200053",
   "metadata": {},
   "source": [
    "## Plot Points Through Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e9d665",
   "metadata": {},
   "source": [
    "This example plots the points extracted from 10 km along the flowine as a time series for each glacier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ce28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "# Loop over glaciers\n",
    "for glacierId, ax in zip(glacierPoints, axes.flatten()):\n",
    "    # Loop over flowlines\n",
    "    for flowlineId in glacierPoints[glacierId]:\n",
    "        #\n",
    "        # Old: interpolate to get results vx(time index) for each point\n",
    "        #vx, vy, v = myVelSeries.interp(*glacierPoints[glacierId][flowlineId], units='km')\n",
    "        #ax.plot(myVelSeries.time, v, marker='o',  linestyle='-', color=flowlineColors[flowlineId],label=f'{flowlineId}')\n",
    "        # New method\n",
    "        myVelSeries.plotPoint(*glacierPoints[glacierId][flowlineId], ax=ax, band='vv',  marker='o',  linestyle='-',\n",
    "                              color=flowlineColors[flowlineId],label=f'{flowlineId}',units='km')\n",
    "    # pretty up plot\n",
    "    myVelSeries.labelPointPlot(ax, title=f'Glacier {glacierId}', plotFontSize=11)\n",
    "    #ax.tick_params(axis='both', labelsize=11)\n",
    "    #ax.legend(ncol=3, loc='upper right', title='Flowline ID')\n",
    "    #ax.set_xlabel('year', fontsize=13)\n",
    "    #ax.set_ylabel('Speed (m/yr)', fontsize=13)\n",
    "    #ax.set_title(f'Glacier {glacierId}')\n",
    "\n",
    "axes[-1, -1].axis('off'); # kill axis with no plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1342f39",
   "metadata": {},
   "source": [
    "## Save the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20abf8",
   "metadata": {},
   "source": [
    "The downloaded subset can be saved in a netcdf and reloaded for to `velSeries` instance for later analysis. Note if the data have been subsetted, the subset will be saved (~30MB in this example). If not, the entire Greeland data set will be saved (370GB). Change `saveData` and `reloadData` below to test this capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ea783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveData = True # Set to True to save data\n",
    "if saveData:\n",
    "    myVelSeries.toNetCDF('Glaciers1-6example.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloadData = True  # Set to True to reload the saved data\n",
    "if reloadData:\n",
    "    myVelCDF = nisar.nisarVelSeries() # Create Series\n",
    "    myVelCDF.readSeriesFromNetCDF('Glaciers1-6example.nc')\n",
    "    #\n",
    "    # repeat the above example with reloaded data\n",
    "    flowlineId ='06'  # Flowline id to plot\n",
    "    fig, axes = plt.subplots(np.ceil(len(myFlowlines)/4).astype(int), 4, figsize=(18, 9))  # Setup plot\n",
    "    # Loop over glaciers\n",
    "    for glacierId, ax in zip(myFlowlines, axes.flatten()):\n",
    "        # loop over each profile by time\n",
    "        for myDate in myVelCDF.time:\n",
    "            myVelSeries.plotProfile(*myFlowlines[glacierId].xy(index=flowlineId, units='km'), date=myDate, label=myDate.year, units='km', ax=ax)\n",
    "        # pretty up plot\n",
    "        myVelSeries.labelProfilePlot(ax, title=f'Glacier {glacierId}', units='km')\n",
    "        ax.legend(ncol=2, loc='upper right')\n",
    "    for ax in axes.flatten()[len(myFlowlines):]:\n",
    "        ax.axis('off'); # kill axis with no plot\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e5132-8cfa-4b59-88b2-631a5b45b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f2e7e-4b77-4152-8e6d-a43085db97ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef22dd5-3cc3-4a4b-8c0c-831e36a9b1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
